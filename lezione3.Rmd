---
title: "Lezione3"
author: "Paolo Bosetti"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
  pdf_document: 
    toc: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(adas.utils)
library(modelr)
knitr::opts_chunk$set(echo = TRUE)
```


# Regressione lineare

## Modello lineare univariato

Generiamo dei dati in modo che sia:

$$
y_i = b x_i + cx_i^2 + \varepsilon_i = y_{\text{nom},i} + \varepsilon_i,~~~\varepsilon_i\sim\mathcal N(0, \sigma^2)
$$
con $a = 0$, $b=2$, $c=0.1$ e $\sigma=2$.


```{r}
set.seed(0)
N <- 100
a <- 0
b <- 2
c <- 0.1

df <- tibble(
  x = seq(-10, 10, length.out = N),
  y_nom = a + b * x + c * x^2,
  y = y_nom + rnorm(N, 0, 2)
)

df %>% 
  ggplot(aes(x = x)) +
  geom_line(aes(y = y_nom), color="red", linetype=2) +
  geom_point(aes(y = y))

```

Si noti che la nominale `y_nom` (cioè $y_{\text{nom},i}$) è normalmente ignota.

Definiamo un modello lineare di secondo grado, dato che la relazione $y=f(x)$ mostra una curvatura. La *formula* R corrispondente a $y=a + bx + cx^2$ è `y ~ x + I(x^2)`. Si noti che `x^2` sarebbe normalmente espanso, secondo l'algebra delle formule, come `x*x`, cioè `x + x + x:x`, cioè in definitiva `x`. Per questo motivo l'espressione quadrata va **protetta** con la funzione identità `I()`, che dice a R: *"non espandere questa espressione ma trattala così com'è"*.

```{r}
(df.lm <- lm(y ~ x + I(x^2), data = df))
```

Si nota che il modello lineare che si ottiene contiene già i valori stimati per i tre coefficienti $a$, $b$ e $c$.

Tutte le informazioni contenute in `df.lm` sono elencabili con `summary()`:

```{r}
attributes(df.lm)
df.lm$coefficients
```

Per regredire polinomi di grado superiore è più rapido usare `poly()`, funzione che costruisce un polinomio del grado desiderato. **Ricordarsi di passare sempre l'opzione `raw=TRUE`**.

```{r}
lm(y ~ poly(x, 10, raw=TRUE), data=df)
```

Ulteriori informazioni sulla regressione si ottengono con `summary()`. In particolare, si nori il corfficielte di determinazione $R^2$ e i *p-value* associati a ciascun coefficiente del modello: coefficienti con *p-value* alto possono essere rimossi dal modello, perché il loro contributo non è statisticamente significativo:

```{r}
summary(df.lm)
```

In questo caso osserviamo che l'intercetta (cioè $a$) può essere rimossa. Per rimuovere l'intercetta in una formula basta sottrarre 1:

```{r}
(df.lm <- lm(y ~ x + I(x^2) - 1, data = df))
```

```{r}
summary(df.lm)
```

È sempre importante verificare la normalità e l'assenza di pattern dei residui:

```{r}
df2 <- df %>% 
  add_residuals(df.lm) %>% 
  add_predictions(df.lm)

df2 %>% 
  ggplot(aes(x=x)) + 
  geom_point(aes(y=y)) + 
  geom_line(aes(y=pred), color="green") + 
  geom_line(aes(y=y_nom), color="red")
```

```{r}
shapiro.test(df2$resid)

df2 %>% 
  ggplot(aes(sample=resid)) + 
  geom_qq() + 
  geom_qq_line(color="red")

df2 %>% 
  ggplot(aes(x = x)) + 
  geom_point(aes(y = resid))
```

Se si desidera solo visualizzare la regressione e la **banda di confidenza** su un grafico, si può ricorrere a `geom_smooth()`:


```{r}
df %>% 
  ggplot(aes(x=x, y=y)) + 
  geom_smooth(
    method="lm",
    formula = y ~ x + I(x^2) - 1,
    level = 0.95
  ) +
  geom_point()
```

## Estrapolazione

Si è visto in teoria che il sovra-adattamento ha due effetti nocivi:

* perdita di generalità
* inaffidabile in estrapolazione

Vediamo quindi cosa succede in estrapolazione, cioè quando si valuta un modello al di fuori dell'intervallo su cui il modello è stato addestrato/regredito.

Anzitutto etichettiamo le osservazioni con una colonna `subset` che valga `"in"` tra $(-7.5, 7.5)$ e `"out"` al di fuori:

```{r}
df3 <- df %>% 
  mutate(
    subset = ifelse(x > -7.5 & x < 7.5, "in", "out")
  )

df3 %>% 
  ggplot(aes(x=x, y=y, color=subset)) + 
  geom_point()
```

Applichiamo la regressione con un modello di grado eccessivo, diciamo 5, ma **solo** sul subset `"in"`:

```{r}
df3 %>% 
  ggplot(aes(x=x, y=y, color=subset)) + 
  geom_point() +
  geom_smooth(
    data = filter(df3, subset=="in"),
    method="lm", 
    formula = y ~ poly(x, 5, raw=T)
  ) 
```

Si noti come è possibile passare a una qualsiasi geometria un dataset diverso da quello base, ricevuto da `ggplot()` via pipe.

Per estendere il modello al di fuori dell'intervallo di addestramento si aggiunge l'argomento `fullrange=TRUE` a `geom_smooth()`:

```{r}
df3 %>% 
  ggplot(aes(x=x, y=y, color=subset)) + 
  geom_point() +
  geom_smooth(
    data = filter(df3, subset=="in"),
    fullrange = T,
    method="lm", 
    formula = y ~ poly(x, 5, raw=T)
  ) 
```

Osserviamo come le bande di confidenza si allarghino molto più rapidamente che nel caso di un adattamento corretto (polinomio di grado 2).

Proviamo ad esagerare con il grado del polinomio. In questo caso è evidente come in estrapolazione il modello regredito "esploda" molto rapidamente, e come all'interno dell'intervallo di addestramento il modello tende a inseguire i punti sperimentali, perdendo quindi di generalità:

```{r}
df3 %>% 
  ggplot(aes(x=x, y=y, color=subset)) + 
  geom_point() +
  geom_smooth(
    data = filter(df3, subset=="in"),
    fullrange = T,
    method="lm", 
    formula = y ~ poly(x, 21, raw=T)
  ) + 
  coord_cartesian(ylim=c(-20,40))
```

Infine, vediamo come ottenere **esplicitamente** l'intervallo di confidenza per il modello lineare `df.lm`. Si usa la funzione `predict()`, che restituisce una tabella che può essere affiancata a `df` con `bind_cols()`:

```{r}
df %>% 
  bind_cols(predict(df.lm, interval="confidence")) %>% 
  ggplot(aes(x=x)) +
  geom_point(aes(y=y)) + 
  geom_line(aes(y=fit)) +
  geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.5)
```


# Cross-validazione

Una tecnica molto comune quando si tratta di confrontare diversi modelli di regressione è la **cross-validazione**. Consiste nel confrontare un indice di qualità della regressione per diversi modelli calcolato sia sui dati utilizzati per la regressione, sia su dati **nuovi**, cioè che non sono stati impiegati per la regressione del modello. Questi dati sono chiamati **dati di validazione**.

Generalmente si procede dividendo l'intero set di dati in un sottoinsieme di dati di **training** e uno di **validazione**, in regime 80/20. Vediamo i dettagli con un esempio:

```{r}
df <- examples_url("kfold.csv") %>% read_csv(show_col_types = FALSE)

df %>% 
  ggplot(aes(x=x, y=y)) +
  geom_point()
```

Osserviamo come il grado opportuno del polinomio da utilizzare per la regressione non sia immediatamente evidente. Infatti, effettuando una regressione con un modello di primo grado e osservando i residui, si vede come c'è evidentemente un *pattern*, ma è difficile dire quale sia il grado del polinomio più adatto:

```{r}
df %>% 
  add_residuals(lm(y~x, data=df)) %>% 
  ggplot(aes(x=x, y=resid)) +
  geom_point()
```

Procediamo con la suddivisione dei dati in un 80% per il training e un 20% per la validazione. Possiamo aggiungere ai dati una colonna con un 80% di valori `TRUE` e il resto `FALSE`, usando la funzione `sample()`:

```{r}
set.seed(1)
df <- df %>% 
  mutate(
    train = sample(c(T, F), n(), T, c(80,20))
  ) 

df %>% 
  ggplot(aes(x=x, y=y, color=train)) + 
  geom_point()
```

Si noti che `sample(c(T, F), n(), T, c(80,20))` restituisce un vettore di valori booleani che *in valore atteso* sono in rapporto 80/20, ma potrebbero essere qualcuno in più o qualcuno in meno. Se vogliamo esattamente il rapporto desiderato possiamo procedere come segue:

```{r}
set.seed(1)
df <- df %>% 
  mutate(train = 1:n() > n()*0.2) %>% 
  mutate(train = sample(train))

df %>% 
  ggplot(aes(x=x, y=y, color=train)) + 
  geom_point()
```

Come si vede, ci sono spesso più modi alternativi per ottenere analoghi risultati. Inoltre, per campioni molto grandi (centinaia di elementi), il primo metodo è in genere sufficiente. In campioni più piccoli, come quello in esame, il campione di validazione consiste in poche osservazioni, quindi vogliamo evitare casi in cui il set di validazione consista in soli due-tre punti, e quindi il secondo sistema è da preferire.

Possiamo ora costruire una **lista di modelli** corrispondenti a polinomi di grado crescente. Usiamo la funzione `map()`:

```{r}
models <- df %>% 
  filter(train) %>% { # il df filtrato viene passato come variabile . a tutte lefunzioni che stanno nel blocco {}
    map(1:10, \(n) lm(y~poly(x, n, raw=T), data=.))
  }
```

Ora `models` è una **lista di modelli lineari**. Da questa lista possiamo estrarre alcuni indici di qualità, come il coefficiente di determinazione. Si ricordi cahe dato un modello lineare `df.lm`, il valore di $R^2$ può essere ottenuto come `summary(df.lm)$r.squared` oppure, usando la libreria `modelr`, come `rsquare(df.lm)`. quindi, usando di nuovo `map`, ci costruiamo una tabella che, per ogni indice del polinomio, riporta il valore di $R^2$ calcolato per ogni modello per i dati di training e per quelli di validazione:

```{r}
tibble(
  n = 1:length(models), # oppure: seq_along(models)
  train = map_dbl(models, \(m) rsquare(m, data=filter(df, train))),
  valid = map_dbl(models, \(m) rsquare(m, data=filter(df, !train)))
) %>% 
  # rendiamola tidy per semplificare il plot:
  pivot_longer(-n, names_to="set", values_to = "rsq") %>% 
  ggplot(aes(x=n, y=rsq, fill=set)) + 
  geom_col(position="dodge")
```

Osserviamo che, mentre il valore di $R^2$ calcolato sui dati di training cresce monotonamente con il grado del polinomio (come atteso), quando $R^2$ è invece calcolato sui dati di validazione raggiunge un massimo tra 3 e 4, dopodiché peggiora.

Più efficiente di $R^2$, per questo tipo di analisi, è il **residuo quadratico medio** RMSE, definito come:

$$
\mathrm{RMSE} = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n}} = \sqrt{\frac{\sum_{i=1}^n \varepsilon_i^2}{n}} 
$$

Il valore di RMSE di un dato modello può essere valutato come `rmse(data, model)`, sempre mediante la libreria `modelr`:

```{r}
tibble(
  n = seq_along(models),
  train = map_dbl(models, \(m) rmse(m, data=filter(df, train))),
  valid = map_dbl(models, \(m) rmse(m, data=filter(df, !train)))
) %>% 
  pivot_longer(-n, names_to="set", values_to = "rsq") %>% 
  ggplot(aes(x=n, y=rsq, fill=set)) + 
  geom_col(position="dodge") + 
  scale_x_continuous(n.breaks=10) # Regolo la griglia dell'asse x
```

Qui gli andamenti sono molto più evidenti: il valore di RMSE valutato sui dati di training decresce con continuità, perché il modello insegue sempre più i singoli punti e quindi i residui diventano sempre più piccoli. Viceversa, quando valutiamo RMSE su dati **che non sono stati usati per la regressione**, esso raggiunge un minimo per il grado 3, poi torna ad aumentare perché i modelli perdono di generalità (*over-fitting*). 

Concludiamo quindi che la regressione con un polinomio di grado 3 sia la più adatta: grado inferiori soffrono di *under-fitting*, gradi superiori di *over-fitting*.

**ESERCIZIO**: selezionare la regressione con un polinomio di grado 3, valutare quali termini siano eventualmente da rimuovere perché non significativi, e verificare i residui per pattern e normalità.



# Dati multivariati

È possibile costruire modelli lineari **multivariati**, cioè che abbiano più di un predittore. In generale, questi modelli rappresentano relazioni in forma di **campi scalari** $\mathbb R^n\rightarrow \mathbb R$ del tipo:

$$
y = f(x_1, x_2,\dots x_n, c_1, c_2, \dots, c_m)
$$

Al solito, costruiamoci un esempio partendo da una relazione nominale:

```{r}
# relazione y = f(x1, x2)
y <- function(x1, x2) 10 - x1 + 0.1*x1^2 + 0.1*(-10*x2 + 1.5*x2^2) + 0.05*x1*x2
```

Avendo due predittori, anziché una sequenza di valori in $\mathbb R$ per un unico predittore, dobbiamo costruire una **griglia** di combinazioni in $\mathbb R^2$:

```{r}
# 50 valori per asse, griglia da 2500 punti:
N <- 50
dfn <- expand.grid(
  x1 = seq(0, 10, length.out = N),
  x2 = seq(0, 10, length.out = N)
) %>% 
  # valutiamo la funzione nominale per ogni punto (x1, x2):
  mutate(
    y = y(x1, x2)
  )

# Grafico a contorno:
dfn %>% 
  ggplot(aes(x=x1, y=x2, z=y)) +
  geom_contour_filled()
```

Ora simuliamo una serie di misurazioni: dalla griglia di 2500 punti ne selezioniamo casualmente 100, ripetendo ogni punto tre volte. In corrispondenza di ogni punto simuliamo poi tre misurazioni (cioè aggiungiamo al valore nominale un disturbo normale):

```{r}
set.seed(10)
Ns <- 100
rpt <- 3

df <- dfn %>% 
  slice_sample(n=Ns) %>% 
  slice(rep(1:n(), each=rpt)) %>% 
  mutate(y = y + rnorm(n(), 0, range(y)/25))
```

Mettiamo in grafico i contorni del modello nominale assieme ai punti campionati, colorati in base al valore misurato:

```{r}
dfn %>% 
  ggplot(aes(x=x1, y=x2)) +
  geom_contour(aes(z=y)) +
  # Attenzione: i dati misurati stanno in un altro dataframe, quindi dobbiamo
  # passarlo alla geometria con il parametro data:
  geom_point(data=df, aes(color=y)) + 
  scale_color_viridis_c() # Cambia la scala colore
```

**NOTA**: i colori in un grafico sono una **scala** e possono essere quindi modificati con le funzioni `scale_color_*()`. In particolare, le scale **viridis** sono scale colore studiate in modo da essere distinguibili anche quando stampate in bianco e nero e anche da soggetti che soffrono di daltonismo.

Ora consideriamo solo i dati misurati (data frame `df`) e creiamo un modello di regressione lineare polinomiale di grado 2, visto che anche guardando i soli punti si osserva una concavità. Avendo due predittori, il modello **completo** (cioè che include tutte le possibili interazioni), è:

$$
y = \mu + c_1x_1 + c_2x_1^2 + c_3x_2 + c_4x_2^2 + c_5x_1x_2^2 + c_6x_1^2x_2 + c_7x_1^2x_2^2
$$

In termini di formula R, l'equivalente è:


```{r}
df.lm <- lm(y ~ poly(x1, 2, raw=T) * poly(x2, 2, raw=T), data=df)
summary(df.lm)
```

Osserviamo che nessuna delle interazioni è significativa, anche se il termine `x1:x2` ha un *p-value* pari a circa il 17%. Prudenzialmente, includiamo anche questa interazione oltre ai termini base (notare il `+` al posto del `*` tra i due polinomi):

```{r}
df.lm <- lm(y ~ poly(x1, 2, raw=T) + poly(x2, 2, raw=T) + x1:x2, data=df)
summary(df.lm)
```

Il sommario conferma la significatività del termine `x1:x2`. Il modello accettato è quindi:

$$
y=\mu + c_1x_1 + c_2x_1^2 + c_3x_2 + c_4x_2^2 + c_5x_1x_2
$$
Ora confrontiamo il modello regredito con il modello nominale, usando due grafici a contorno:

```{r}
dfn %>% 
  add_predictions(df.lm) %>% 
  ggplot(aes(x=x1, y=x2, z=y)) + 
  geom_contour_filled(bins=10) + 
  geom_contour(aes(z=pred), bins=10)
```

**ESERCIZIO**: analizzare i residui (pattern e normalità).


# Regressione non lineare

## Regressione ai minimi quadrati

Se il modello da regredire non è lineare nei coefficienti (ad esempio perché è definito per parti), non è possibile effettuare una regressione con `lm`. In questi casi bisogna usare la **regressione non-lineare ai minimi quadrati** fornita in R dalla funzione `nls()` (*Nonlinear Least-Squares*).

Come esempio consideriamo il caso di una misura di durezza mediante **indentazione strumentata**. Abbiamo un dispositivo che registra la forza applicata da un penetratore pressato sulla superficie piana di un materiale in funzione del tempo (o dello spostamento verticale). Il segnale della forza avrà inizialmente (prima del contatto) un valore nominale costante che chiamiamo `bias`, poi al tempo di contatto `t0` il carico comincia a aumentare seguendo una legge quadratica, con tangenza tra il tratto costante e quello parabolico. Possiamo definire una funzione R parametrica in questo modo:

```{r}
f <- function(t, t0=0, bias=0, a=1) {
  b <- -2 * a * t0
  c <- bias + a * t0^2
  y <- a * t^2 + b * t + c
  return(ifelse(t < t0, bias, y))
}
```

Possiamo verificare il grafico di questa funzione creando una tabella di dati di appoggio:

```{r}
tibble(
  t = seq(-2.5, 5, length.out=100),
  f = f(t, -1, 1, 0.5)
) %>% 
  ggplot(aes(x=t, y=f)) + 
  geom_line()
```

Possiamo anche creare direttamente il grafico con la geometria `geom_function()`. In questo caso dobbiamo esplicitamente specificare l'intervallo X con `xlim()`, dato che non essendoci un data frame di appoggio esso non è definito:

```{r}
ggplot() +
  geom_function(fun=f, args=list(bias=1, t0=-1, a=0.5)) +
  geom_vline(xintercept = -1, linetype=2) + # linea verticale a x=-1
  xlim(c(-2.5,5)) +
  labs(x="Tempo (s)", y="Carico (N)")
```

Ora possiamo simulare l'esperimento creando un set di dati e aggiungendo del rumore:

```{r}
set.seed(1)
data <- tibble(
  t = seq(-10, 10, length.out=100),
  yn = f(t, 2.5, 3, 1),
  y = yn + rnorm(length(t), 0, 2)
)

data %>% 
  ggplot(aes(x=t, y=y)) + 
  geom_point()
```
È il caso di una regressione di un *modello fisico*, che però non è lineare (e non è nemmeno analitico, essendo definito per parti).

La regressione non è quindi realizzabile né con `lm`, né con `glm`. È necessario ricorrere alla **regressione ai minimi quadrati** con `nlm`. Analogamente a `lm()`, questa funzione di regressione vuole una formula e il set di dati su cui operare. In più, però, è necessario specificare anche la lista di **condizioni iniziali** (*guess*) di avvio dell'ottimizzazione, come lista passata al parametro `start`:

```{r}
data.nls <- nls(y~f(t, t0, b, a), 
    data=data,
    start=list(
      t0 = 0,
      b = 0,
      a = 10
    ),
    trace=TRUE)

summary(data.nls)
```

L'opzione `trace=TRUE` fornisce in output i passi iterativi e i parametri di ottimizzazione. Si noti inoltre che la formula deve contenere una funzione del (o dei) predittori e dei coefficienti del modello.

I coefficienti risultano:

```{r}
coefficients(data.nls)
```

Il modello regredito può essere al solito messo in grafico con `add_predictions()`:

```{r}
data %>% 
  add_predictions(data.nls) %>% 
  ggplot(aes(x=t)) + 
  geom_point(aes(y=y)) + 
  geom_line(aes(y=pred), color="red")
```

**ESERCIZIO**: completare l'esempio con l'analisi dei residui.

A questo punto, il diagramma di carico può essere: 

* compensato per il termine `bias` (che rappresenta la *tara*, cioè il carico percepito dal sensore di forza quando non c'è contatto)
* traslato a zero nel momento di contatto
* depurato delle misure prima del contatto

```{r}
coef <- coefficients(data.nls)
data %>% 
  add_predictions(data.nls) %>% 
  mutate(
    y = y - coef["b"],
    pred = pred - coef["b"],
    t = t - coef["t0"]
  ) %>% 
  filter(t > 0) %>% 
  ggplot(aes(x=t)) + 
  geom_point(aes(y=y)) + 
  geom_line(aes(y=pred), color="red") +
  labs(x="tempo (s)", y="Carico (N)")
```

Si noti che per i modelli `nls` non è possibile ottenere direttamente gli intervalli di confidenza:

```{r}
predict(data.nls, interval="confidence")
```

Cioè, anche se richiediamo la banda di confidenza, la funzione `predict()` applicata a un modello `nls` restituisce **solo** la predizione. Ciò è dovuto al fatto che, se per i modelli lineari la banda di confidenza può essere calcolata analiticamente, per i modelli non-lineari non è possibile farlo.

Tuttavia, è possibile *simulare* la banda di confidenza con metodi numerici/iterativi, ad esempio usando `predFit()` nella libreria `investr`:

```{r}
library(investr)
data %>%
  bind_cols(
    predFit(
      data.nls, 
      newdata=., # ricorda: . rappresenta i dati ricevuti via pipe
      level=0.99,
      interval="confidence"
    )
  ) %>% 
  mutate(
    t = t - coef["t0"],
    across(y:upr, \(f) f - coef["b"])
  ) %>% 
  filter(t > 0) %>% 
  ggplot(aes(x=t)) + 
  geom_point(aes(y=y)) + 
  geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=1/3) +
  geom_line(aes(y=fit), color="red") +
  labs(x="tempo (s)", y="Carico (N)", 
       title="Conf.level: 0.99%")
```

